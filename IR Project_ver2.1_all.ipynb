{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a80330a-41a9-4478-a6cf-43ce6b1a576b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2629704-df95-44dd-a5f2-01a1a2da5a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/edwin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edwin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "from pyterrier.measures import *\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import pickle\n",
    "# es_stemer = SnowballStemmer('spanish')\n",
    "# es_stopwords = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d5b9a-3740-44ab-b769-e6bbe110c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c46823-b42c-4ab7-95e3-4dec72b31fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c587631-97d9-4883-b221-5ac852442353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1faaa-39cc-4f39-9644-2faa34c1f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696cf7e-ba3e-494e-b9ca-713ac4559fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def IndexCreator(dataset,filename, tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None):\n",
    "    if not os.path.exists(filename):\n",
    "        indexer = pt.IterDictIndexer(filename, \n",
    "            stemmer=stemmer, stopwords=stopwords, # Removes the default PorterStemmer (English)\n",
    "            tokeniser=tokeniser) # Replaces the default EnglishTokeniser, which makes assumptions specific to English\n",
    "        index_ = indexer.index(dataset.get_corpus_iter())\n",
    "    else:\n",
    "        index_ = pt.IndexRef.of(filename)\n",
    "    return index_\n",
    "\n",
    "\n",
    "def custom_preprocess(text):\n",
    "    toks = word_tokenize(text) # tokenize\n",
    "    toks = [t for t in toks if t.lower() not in custom_stopwords] # remove stop words\n",
    "    toks = [custom_stemmer.stem(t) for t in toks] # stem\n",
    "    return ' '.join(toks) # combine toks back into a string\n",
    "\n",
    "\n",
    "\n",
    "# Custom Preprocessing\n",
    "# NB: This custom pre-processing ends up being considerably slower than using Terrier's built-in processor,\n",
    "# so we use the multiprocessing package to parallelize (400 docs/s vs 2000 docs/s).\n",
    "def map_doc(document,custom_preprocess=custom_preprocess):\n",
    "    # this function replaces the document text with the version that uses our custom pre-processing\n",
    "    return {\n",
    "        'docno': document['docno'],\n",
    "        'text': custom_preprocess(document['text'])\n",
    "    }\n",
    "\n",
    "def CustomIndexCreator(filename, tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None,mapper=map_doc):\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        indexer = pt.IterDictIndexer(filename, \n",
    "            stemmer=None, stopwords=None,  # Disable the default PorterStemmer (English)\n",
    "            tokeniser=tokeniser) # Replaces the default EnglishTokeniser, which makes assumptions specific to English\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            index_custom = indexer.index(pool.imap(mapper, dataset.get_corpus_iter()))\n",
    "    else:\n",
    "        index_custom = pt.IndexRef.of(filename)\n",
    "    return index_custom\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa268497-8d79-47c3-a1c2-c7e800eba96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "1:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/en59k/test\",\n",
    "        \"lang\":\"english\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"EnglishSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "2:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/es13k/test\",\n",
    "        \"lang\":\"spanish\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"SpanishSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "3:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/fr14k/test\",\n",
    "        \"lang\":\"french\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"FrenchSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "4:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/it16k/test\",\n",
    "        \"lang\":\"italian\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"ItalianSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    }\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea8a312-9bd9-47eb-baea-bbd998f07f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55dd3c-9c48-4dcf-8795-f05dae746c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "result={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc4b2f-3799-4f75-a631-484358267473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0f161-62cb-453a-9a70-5d470768614f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b66950-8935-45ba-b602-4d1ebacaba3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████▊                    | 1/4 [20:29<1:01:28, 1229.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████               | 2/4 [34:13<33:01, 990.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████▌       | 3/4 [47:49<15:11, 911.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 4/4 [1:01:54<00:00, 928.68s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm(range(1,5)):\n",
    "    print(i)\n",
    "    config_data=config[i]\n",
    "    result[config_data[\"lang\"]]={}\n",
    "\n",
    "    custom_stemmer=None\n",
    "    custom_stopwords=None\n",
    "\n",
    "    dataset = pt.get_dataset(config_data[\"dataset\"])\n",
    "    custom_stemmer = SnowballStemmer(config_data[\"lang\"])\n",
    "    custom_stopwords = set(stopwords.words(config_data[\"lang\"]))\n",
    "    def custom_preprocess(text):\n",
    "        toks = word_tokenize(text) # tokenize\n",
    "        toks = [t for t in toks if t.lower() not in custom_stopwords] # remove stop words\n",
    "        toks = [custom_stemmer.stem(t) for t in toks] # stem\n",
    "        return ' '.join(toks) # combine toks back into a string\n",
    "\n",
    "    def map_doc(document,custom_preprocess=custom_preprocess):\n",
    "        # this function replaces the document text with the version that uses our custom pre-processing\n",
    "        return {\n",
    "            'docno': document['docno'],\n",
    "            'text': custom_preprocess(document['text'])\n",
    "        }\n",
    "\n",
    "    index_nostem = IndexCreator(dataset=dataset,filename=f'./wikir-{config_data[\"lang\"]}-nostem',\n",
    "                                tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None)\n",
    "    index_stem=IndexCreator(dataset=dataset,filename=f'./wikir-{config_data[\"lang\"]}-stem', \n",
    "                            tokeniser=\"UTFTokeniser\",stemmer=config_data[\"stemmer\"],stopwords=None)\n",
    "    index_custom = CustomIndexCreator(filename=f'./wikir-{config_data[\"lang\"]}-custom_stem', tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None,mapper=map_doc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    bm25_nostem = pt.BatchRetrieve(index_nostem, wmodel='BM25')\n",
    "    bm25_stem = pt.BatchRetrieve(index_stem, wmodel='BM25')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    bm25_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='BM25')\n",
    "\n",
    "\n",
    "    tfidf_nostem = pt.BatchRetrieve(index_nostem, wmodel='TF_IDF')\n",
    "    tfidf_stem = pt.BatchRetrieve(index_stem, wmodel='TF_IDF')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    tfidf_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='TF_IDF')\n",
    "\n",
    "\n",
    "\n",
    "    bb2_nostem = pt.BatchRetrieve(index_nostem, wmodel='BB2')\n",
    "    bb2_stem = pt.BatchRetrieve(index_stem, wmodel='BB2')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    bb2_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='BB2')\n",
    "\n",
    "\n",
    "\n",
    "    # http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html\n",
    "\n",
    "\n",
    "    title_qrels = dataset.get_qrels().copy()\n",
    "    title_qrels.loc[title_qrels.label < 2, 'label'] = 0\n",
    "    title_result=pt.Experiment(\n",
    "        [bm25_nostem, bm25_stem, bm25_custom,tfidf_nostem, tfidf_stem, tfidf_custom,bb2_nostem, bb2_stem, bb2_custom],\n",
    "        dataset.get_topics(),\n",
    "        title_qrels,\n",
    "        [nDCG@5, nDCG@10, nDCG@20,\"P_5\",\"P_10\",\"P_15\",\"P_20\",\"P_100\",\"recip_rank\",\"map\", NumQ],\n",
    "        names=['BM25 nostem', 'BM25 stem', 'BM25 custom','TFIDF nostem', 'TFIDF stem', 'TFIDF custom','BB2 nostem', 'BB2 stem', 'BB2 custom'],\n",
    "        round=4\n",
    "    )\n",
    "\n",
    "\n",
    "    doc_result=pt.Experiment(\n",
    "        [bm25_nostem, bm25_stem, bm25_custom,tfidf_nostem, tfidf_stem, tfidf_custom,bb2_nostem, bb2_stem, bb2_custom],\n",
    "        dataset.get_topics(),\n",
    "        dataset.get_qrels(),\n",
    "        [nDCG@5, nDCG@10, nDCG@20,\"P_5\",\"P_10\",\"P_15\",\"P_20\",\"P_100\",\"recip_rank\",\"map\", NumQ],\n",
    "        names=['BM25 nostem', 'BM25 stem', 'BM25 custom','TFIDF nostem', 'TFIDF stem', 'TFIDF custom','BB2 nostem', 'BB2 stem', 'BB2 custom'],\n",
    "        round=4\n",
    "    )\n",
    "    \n",
    "    \n",
    "    result[config_data[\"lang\"]]={\"title_result\":title_result.to_dict(),\"doc_result\":doc_result.to_dict()}\n",
    "    \n",
    "    with open('./data_result.pkl', 'wb') as f:\n",
    "        pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336cb78-d579-49ce-8307-452c12514544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7c83ba7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a3286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/edwin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edwin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "from pyterrier.measures import *\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import pickle\n",
    "# es_stemer = SnowballStemmer('spanish')\n",
    "# es_stopwords = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0963c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867d7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936a8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a253b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def IndexCreator(dataset,filename, tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None):\n",
    "    if not os.path.exists(filename):\n",
    "        indexer = pt.IterDictIndexer(filename, \n",
    "            stemmer=stemmer, stopwords=stopwords, # Removes the default PorterStemmer (English)\n",
    "            tokeniser=tokeniser) # Replaces the default EnglishTokeniser, which makes assumptions specific to English\n",
    "        index_ = indexer.index(dataset.get_corpus_iter())\n",
    "    else:\n",
    "        index_ = pt.IndexRef.of(filename)\n",
    "    return index_\n",
    "\n",
    "\n",
    "def custom_preprocess(text):\n",
    "    toks = word_tokenize(text) # tokenize\n",
    "    toks = [t for t in toks if t.lower() not in custom_stopwords] # remove stop words\n",
    "    toks = [custom_stemmer.stem(t) for t in toks] # stem\n",
    "    return ' '.join(toks) # combine toks back into a string\n",
    "\n",
    "\n",
    "\n",
    "# Custom Preprocessing\n",
    "# NB: This custom pre-processing ends up being considerably slower than using Terrier's built-in processor,\n",
    "# so we use the multiprocessing package to parallelize (400 docs/s vs 2000 docs/s).\n",
    "def map_doc(document,custom_preprocess=custom_preprocess):\n",
    "    # this function replaces the document text with the version that uses our custom pre-processing\n",
    "    return {\n",
    "        'docno': document['docno'],\n",
    "        'text': custom_preprocess(document['text'])\n",
    "    }\n",
    "\n",
    "def CustomIndexCreator(filename, tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None,mapper=map_doc):\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        indexer = pt.IterDictIndexer(filename, \n",
    "            stemmer=None, stopwords=None,  # Disable the default PorterStemmer (English)\n",
    "            tokeniser=tokeniser) # Replaces the default EnglishTokeniser, which makes assumptions specific to English\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            index_custom = indexer.index(pool.imap(mapper, dataset.get_corpus_iter()))\n",
    "    else:\n",
    "        index_custom = pt.IndexRef.of(filename)\n",
    "    return index_custom\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "1:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/en59k/test\",\n",
    "        \"lang\":\"english\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"EnglishSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "2:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/es13k/test\",\n",
    "        \"lang\":\"spanish\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"SpanishSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "3:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/fr14k/test\",\n",
    "        \"lang\":\"french\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"FrenchSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "4:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/it16k/test\",\n",
    "        \"lang\":\"italian\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"ItalianSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    }\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91620b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2761b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a5ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859cee14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211cb97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████▊                    | 1/4 [20:29<1:01:28, 1229.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████               | 2/4 [34:13<33:01, 990.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████▌       | 3/4 [47:49<15:11, 911.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 4/4 [1:01:54<00:00, 928.68s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm(range(1,5)):\n",
    "    print(i)\n",
    "    config_data=config[i]\n",
    "    result[config_data[\"lang\"]]={}\n",
    "\n",
    "    custom_stemmer=None\n",
    "    custom_stopwords=None\n",
    "\n",
    "    dataset = pt.get_dataset(config_data[\"dataset\"])\n",
    "    custom_stemmer = SnowballStemmer(config_data[\"lang\"])\n",
    "    custom_stopwords = set(stopwords.words(config_data[\"lang\"]))\n",
    "    def custom_preprocess(text):\n",
    "        toks = word_tokenize(text) # tokenize\n",
    "        toks = [t for t in toks if t.lower() not in custom_stopwords] # remove stop words\n",
    "        toks = [custom_stemmer.stem(t) for t in toks] # stem\n",
    "        return ' '.join(toks) # combine toks back into a string\n",
    "\n",
    "    def map_doc(document,custom_preprocess=custom_preprocess):\n",
    "        # this function replaces the document text with the version that uses our custom pre-processing\n",
    "        return {\n",
    "            'docno': document['docno'],\n",
    "            'text': custom_preprocess(document['text'])\n",
    "        }\n",
    "\n",
    "    index_nostem = IndexCreator(dataset=dataset,filename=f'./wikir-{config_data[\"lang\"]}-nostem',\n",
    "                                tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None)\n",
    "    index_stem=IndexCreator(dataset=dataset,filename=f'./wikir-{config_data[\"lang\"]}-stem', \n",
    "                            tokeniser=\"UTFTokeniser\",stemmer=config_data[\"stemmer\"],stopwords=None)\n",
    "    index_custom = CustomIndexCreator(filename=f'./wikir-{config_data[\"lang\"]}-custom_stem', tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None,mapper=map_doc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    bm25_nostem = pt.BatchRetrieve(index_nostem, wmodel='BM25')\n",
    "    bm25_stem = pt.BatchRetrieve(index_stem, wmodel='BM25')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    bm25_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='BM25')\n",
    "\n",
    "\n",
    "    tfidf_nostem = pt.BatchRetrieve(index_nostem, wmodel='TF_IDF')\n",
    "    tfidf_stem = pt.BatchRetrieve(index_stem, wmodel='TF_IDF')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    tfidf_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='TF_IDF')\n",
    "\n",
    "\n",
    "\n",
    "    bb2_nostem = pt.BatchRetrieve(index_nostem, wmodel='BB2')\n",
    "    bb2_stem = pt.BatchRetrieve(index_stem, wmodel='BB2')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    bb2_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='BB2')\n",
    "\n",
    "\n",
    "\n",
    "    # http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html\n",
    "\n",
    "\n",
    "    title_qrels = dataset.get_qrels().copy()\n",
    "    title_qrels.loc[title_qrels.label < 2, 'label'] = 0\n",
    "    title_result=pt.Experiment(\n",
    "        [bm25_nostem, bm25_stem, bm25_custom,tfidf_nostem, tfidf_stem, tfidf_custom,bb2_nostem, bb2_stem, bb2_custom],\n",
    "        dataset.get_topics(),\n",
    "        title_qrels,\n",
    "        [nDCG@5, nDCG@10, nDCG@20,\"P_5\",\"P_10\",\"P_15\",\"P_20\",\"P_100\",\"recip_rank\",\"map\", NumQ],\n",
    "        names=['BM25 nostem', 'BM25 stem', 'BM25 custom','TFIDF nostem', 'TFIDF stem', 'TFIDF custom','BB2 nostem', 'BB2 stem', 'BB2 custom'],\n",
    "        round=4\n",
    "    )\n",
    "\n",
    "\n",
    "    doc_result=pt.Experiment(\n",
    "        [bm25_nostem, bm25_stem, bm25_custom,tfidf_nostem, tfidf_stem, tfidf_custom,bb2_nostem, bb2_stem, bb2_custom],\n",
    "        dataset.get_topics(),\n",
    "        dataset.get_qrels(),\n",
    "        [nDCG@5, nDCG@10, nDCG@20,\"P_5\",\"P_10\",\"P_15\",\"P_20\",\"P_100\",\"recip_rank\",\"map\", NumQ],\n",
    "        names=['BM25 nostem', 'BM25 stem', 'BM25 custom','TFIDF nostem', 'TFIDF stem', 'TFIDF custom','BB2 nostem', 'BB2 stem', 'BB2 custom'],\n",
    "        round=4\n",
    "    )\n",
    "    \n",
    "    \n",
    "    result[config_data[\"lang\"]]={\"title_result\":title_result.to_dict(),\"doc_result\":doc_result.to_dict()}\n",
    "    \n",
    "    with open('./data_result.pkl', 'wb') as f:\n",
    "        pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1aa19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "459d66fe",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f690f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/edwin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edwin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "from pyterrier.measures import *\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import pickle\n",
    "# es_stemer = SnowballStemmer('spanish')\n",
    "# es_stopwords = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f5401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be657560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ecdd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def IndexCreator(dataset,filename, tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None):\n",
    "    if not os.path.exists(filename):\n",
    "        indexer = pt.IterDictIndexer(filename, \n",
    "            stemmer=stemmer, stopwords=stopwords, # Removes the default PorterStemmer (English)\n",
    "            tokeniser=tokeniser) # Replaces the default EnglishTokeniser, which makes assumptions specific to English\n",
    "        index_ = indexer.index(dataset.get_corpus_iter())\n",
    "    else:\n",
    "        index_ = pt.IndexRef.of(filename)\n",
    "    return index_\n",
    "\n",
    "\n",
    "def custom_preprocess(text):\n",
    "    toks = word_tokenize(text) # tokenize\n",
    "    toks = [t for t in toks if t.lower() not in custom_stopwords] # remove stop words\n",
    "    toks = [custom_stemmer.stem(t) for t in toks] # stem\n",
    "    return ' '.join(toks) # combine toks back into a string\n",
    "\n",
    "\n",
    "\n",
    "# Custom Preprocessing\n",
    "# NB: This custom pre-processing ends up being considerably slower than using Terrier's built-in processor,\n",
    "# so we use the multiprocessing package to parallelize (400 docs/s vs 2000 docs/s).\n",
    "def map_doc(document,custom_preprocess=custom_preprocess):\n",
    "    # this function replaces the document text with the version that uses our custom pre-processing\n",
    "    return {\n",
    "        'docno': document['docno'],\n",
    "        'text': custom_preprocess(document['text'])\n",
    "    }\n",
    "\n",
    "def CustomIndexCreator(filename, tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None,mapper=map_doc):\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        indexer = pt.IterDictIndexer(filename, \n",
    "            stemmer=None, stopwords=None,  # Disable the default PorterStemmer (English)\n",
    "            tokeniser=tokeniser) # Replaces the default EnglishTokeniser, which makes assumptions specific to English\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            index_custom = indexer.index(pool.imap(mapper, dataset.get_corpus_iter()))\n",
    "    else:\n",
    "        index_custom = pt.IndexRef.of(filename)\n",
    "    return index_custom\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95eaf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "1:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/en59k/test\",\n",
    "        \"lang\":\"english\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"EnglishSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "2:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/es13k/test\",\n",
    "        \"lang\":\"spanish\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"SpanishSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "3:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/fr14k/test\",\n",
    "        \"lang\":\"french\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"FrenchSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "4:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/it16k/test\",\n",
    "        \"lang\":\"italian\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"ItalianSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    }\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab08e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "result={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28bae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64fed64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████▊                    | 1/4 [20:29<1:01:28, 1229.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████               | 2/4 [34:13<33:01, 990.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████▌       | 3/4 [47:49<15:11, 911.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 4/4 [1:01:54<00:00, 928.68s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm(range(1,5)):\n",
    "    print(i)\n",
    "    config_data=config[i]\n",
    "    result[config_data[\"lang\"]]={}\n",
    "\n",
    "    custom_stemmer=None\n",
    "    custom_stopwords=None\n",
    "\n",
    "    dataset = pt.get_dataset(config_data[\"dataset\"])\n",
    "    custom_stemmer = SnowballStemmer(config_data[\"lang\"])\n",
    "    custom_stopwords = set(stopwords.words(config_data[\"lang\"]))\n",
    "    def custom_preprocess(text):\n",
    "        toks = word_tokenize(text) # tokenize\n",
    "        toks = [t for t in toks if t.lower() not in custom_stopwords] # remove stop words\n",
    "        toks = [custom_stemmer.stem(t) for t in toks] # stem\n",
    "        return ' '.join(toks) # combine toks back into a string\n",
    "\n",
    "    def map_doc(document,custom_preprocess=custom_preprocess):\n",
    "        # this function replaces the document text with the version that uses our custom pre-processing\n",
    "        return {\n",
    "            'docno': document['docno'],\n",
    "            'text': custom_preprocess(document['text'])\n",
    "        }\n",
    "\n",
    "    index_nostem = IndexCreator(dataset=dataset,filename=f'./wikir-{config_data[\"lang\"]}-nostem',\n",
    "                                tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None)\n",
    "    index_stem=IndexCreator(dataset=dataset,filename=f'./wikir-{config_data[\"lang\"]}-stem', \n",
    "                            tokeniser=\"UTFTokeniser\",stemmer=config_data[\"stemmer\"],stopwords=None)\n",
    "    index_custom = CustomIndexCreator(filename=f'./wikir-{config_data[\"lang\"]}-custom_stem', tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None,mapper=map_doc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    bm25_nostem = pt.BatchRetrieve(index_nostem, wmodel='BM25')\n",
    "    bm25_stem = pt.BatchRetrieve(index_stem, wmodel='BM25')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    bm25_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='BM25')\n",
    "\n",
    "\n",
    "    tfidf_nostem = pt.BatchRetrieve(index_nostem, wmodel='TF_IDF')\n",
    "    tfidf_stem = pt.BatchRetrieve(index_stem, wmodel='TF_IDF')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    tfidf_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='TF_IDF')\n",
    "\n",
    "\n",
    "\n",
    "    bb2_nostem = pt.BatchRetrieve(index_nostem, wmodel='BB2')\n",
    "    bb2_stem = pt.BatchRetrieve(index_stem, wmodel='BB2')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    bb2_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='BB2')\n",
    "\n",
    "\n",
    "\n",
    "    # http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html\n",
    "\n",
    "\n",
    "    title_qrels = dataset.get_qrels().copy()\n",
    "    title_qrels.loc[title_qrels.label < 2, 'label'] = 0\n",
    "    title_result=pt.Experiment(\n",
    "        [bm25_nostem, bm25_stem, bm25_custom,tfidf_nostem, tfidf_stem, tfidf_custom,bb2_nostem, bb2_stem, bb2_custom],\n",
    "        dataset.get_topics(),\n",
    "        title_qrels,\n",
    "        [nDCG@5, nDCG@10, nDCG@20,\"P_5\",\"P_10\",\"P_15\",\"P_20\",\"P_100\",\"recip_rank\",\"map\", NumQ],\n",
    "        names=['BM25 nostem', 'BM25 stem', 'BM25 custom','TFIDF nostem', 'TFIDF stem', 'TFIDF custom','BB2 nostem', 'BB2 stem', 'BB2 custom'],\n",
    "        round=4\n",
    "    )\n",
    "\n",
    "\n",
    "    doc_result=pt.Experiment(\n",
    "        [bm25_nostem, bm25_stem, bm25_custom,tfidf_nostem, tfidf_stem, tfidf_custom,bb2_nostem, bb2_stem, bb2_custom],\n",
    "        dataset.get_topics(),\n",
    "        dataset.get_qrels(),\n",
    "        [nDCG@5, nDCG@10, nDCG@20,\"P_5\",\"P_10\",\"P_15\",\"P_20\",\"P_100\",\"recip_rank\",\"map\", NumQ],\n",
    "        names=['BM25 nostem', 'BM25 stem', 'BM25 custom','TFIDF nostem', 'TFIDF stem', 'TFIDF custom','BB2 nostem', 'BB2 stem', 'BB2 custom'],\n",
    "        round=4\n",
    "    )\n",
    "    \n",
    "    \n",
    "    result[config_data[\"lang\"]]={\"title_result\":title_result.to_dict(),\"doc_result\":doc_result.to_dict()}\n",
    "    \n",
    "    with open('./data_result.pkl', 'wb') as f:\n",
    "        pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbed063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeb03919",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febfba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/edwin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edwin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "from pyterrier.measures import *\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import pickle\n",
    "# es_stemer = SnowballStemmer('spanish')\n",
    "# es_stopwords = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def IndexCreator(dataset,filename, tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None):\n",
    "    if not os.path.exists(filename):\n",
    "        indexer = pt.IterDictIndexer(filename, \n",
    "            stemmer=stemmer, stopwords=stopwords, # Removes the default PorterStemmer (English)\n",
    "            tokeniser=tokeniser) # Replaces the default EnglishTokeniser, which makes assumptions specific to English\n",
    "        index_ = indexer.index(dataset.get_corpus_iter())\n",
    "    else:\n",
    "        index_ = pt.IndexRef.of(filename)\n",
    "    return index_\n",
    "\n",
    "\n",
    "def custom_preprocess(text):\n",
    "    toks = word_tokenize(text) # tokenize\n",
    "    toks = [t for t in toks if t.lower() not in custom_stopwords] # remove stop words\n",
    "    toks = [custom_stemmer.stem(t) for t in toks] # stem\n",
    "    return ' '.join(toks) # combine toks back into a string\n",
    "\n",
    "\n",
    "\n",
    "# Custom Preprocessing\n",
    "# NB: This custom pre-processing ends up being considerably slower than using Terrier's built-in processor,\n",
    "# so we use the multiprocessing package to parallelize (400 docs/s vs 2000 docs/s).\n",
    "def map_doc(document,custom_preprocess=custom_preprocess):\n",
    "    # this function replaces the document text with the version that uses our custom pre-processing\n",
    "    return {\n",
    "        'docno': document['docno'],\n",
    "        'text': custom_preprocess(document['text'])\n",
    "    }\n",
    "\n",
    "def CustomIndexCreator(filename, tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None,mapper=map_doc):\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        indexer = pt.IterDictIndexer(filename, \n",
    "            stemmer=None, stopwords=None,  # Disable the default PorterStemmer (English)\n",
    "            tokeniser=tokeniser) # Replaces the default EnglishTokeniser, which makes assumptions specific to English\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            index_custom = indexer.index(pool.imap(mapper, dataset.get_corpus_iter()))\n",
    "    else:\n",
    "        index_custom = pt.IndexRef.of(filename)\n",
    "    return index_custom\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "1:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/en59k/test\",\n",
    "        \"lang\":\"english\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"EnglishSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "2:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/es13k/test\",\n",
    "        \"lang\":\"spanish\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"SpanishSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "3:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/fr14k/test\",\n",
    "        \"lang\":\"french\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"FrenchSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "4:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/it16k/test\",\n",
    "        \"lang\":\"italian\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"ItalianSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    }\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeacc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ffaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6f5a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████▊                    | 1/4 [20:29<1:01:28, 1229.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████               | 2/4 [34:13<33:01, 990.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████▌       | 3/4 [47:49<15:11, 911.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 4/4 [1:01:54<00:00, 928.68s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm(range(1,5)):\n",
    "    print(i)\n",
    "    config_data=config[i]\n",
    "    result[config_data[\"lang\"]]={}\n",
    "\n",
    "    custom_stemmer=None\n",
    "    custom_stopwords=None\n",
    "\n",
    "    dataset = pt.get_dataset(config_data[\"dataset\"])\n",
    "    custom_stemmer = SnowballStemmer(config_data[\"lang\"])\n",
    "    custom_stopwords = set(stopwords.words(config_data[\"lang\"]))\n",
    "    def custom_preprocess(text):\n",
    "        toks = word_tokenize(text) # tokenize\n",
    "        toks = [t for t in toks if t.lower() not in custom_stopwords] # remove stop words\n",
    "        toks = [custom_stemmer.stem(t) for t in toks] # stem\n",
    "        return ' '.join(toks) # combine toks back into a string\n",
    "\n",
    "    def map_doc(document,custom_preprocess=custom_preprocess):\n",
    "        # this function replaces the document text with the version that uses our custom pre-processing\n",
    "        return {\n",
    "            'docno': document['docno'],\n",
    "            'text': custom_preprocess(document['text'])\n",
    "        }\n",
    "\n",
    "    index_nostem = IndexCreator(dataset=dataset,filename=f'./wikir-{config_data[\"lang\"]}-nostem',\n",
    "                                tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None)\n",
    "    index_stem=IndexCreator(dataset=dataset,filename=f'./wikir-{config_data[\"lang\"]}-stem', \n",
    "                            tokeniser=\"UTFTokeniser\",stemmer=config_data[\"stemmer\"],stopwords=None)\n",
    "    index_custom = CustomIndexCreator(filename=f'./wikir-{config_data[\"lang\"]}-custom_stem', tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None,mapper=map_doc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    bm25_nostem = pt.BatchRetrieve(index_nostem, wmodel='BM25')\n",
    "    bm25_stem = pt.BatchRetrieve(index_stem, wmodel='BM25')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    bm25_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='BM25')\n",
    "\n",
    "\n",
    "    tfidf_nostem = pt.BatchRetrieve(index_nostem, wmodel='TF_IDF')\n",
    "    tfidf_stem = pt.BatchRetrieve(index_stem, wmodel='TF_IDF')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    tfidf_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='TF_IDF')\n",
    "\n",
    "\n",
    "\n",
    "    bb2_nostem = pt.BatchRetrieve(index_nostem, wmodel='BB2')\n",
    "    bb2_stem = pt.BatchRetrieve(index_stem, wmodel='BB2')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    bb2_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='BB2')\n",
    "\n",
    "\n",
    "\n",
    "    # http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html\n",
    "\n",
    "\n",
    "    title_qrels = dataset.get_qrels().copy()\n",
    "    title_qrels.loc[title_qrels.label < 2, 'label'] = 0\n",
    "    title_result=pt.Experiment(\n",
    "        [bm25_nostem, bm25_stem, bm25_custom,tfidf_nostem, tfidf_stem, tfidf_custom,bb2_nostem, bb2_stem, bb2_custom],\n",
    "        dataset.get_topics(),\n",
    "        title_qrels,\n",
    "        [nDCG@5, nDCG@10, nDCG@20,\"P_5\",\"P_10\",\"P_15\",\"P_20\",\"P_100\",\"recip_rank\",\"map\", NumQ],\n",
    "        names=['BM25 nostem', 'BM25 stem', 'BM25 custom','TFIDF nostem', 'TFIDF stem', 'TFIDF custom','BB2 nostem', 'BB2 stem', 'BB2 custom'],\n",
    "        round=4\n",
    "    )\n",
    "\n",
    "\n",
    "    doc_result=pt.Experiment(\n",
    "        [bm25_nostem, bm25_stem, bm25_custom,tfidf_nostem, tfidf_stem, tfidf_custom,bb2_nostem, bb2_stem, bb2_custom],\n",
    "        dataset.get_topics(),\n",
    "        dataset.get_qrels(),\n",
    "        [nDCG@5, nDCG@10, nDCG@20,\"P_5\",\"P_10\",\"P_15\",\"P_20\",\"P_100\",\"recip_rank\",\"map\", NumQ],\n",
    "        names=['BM25 nostem', 'BM25 stem', 'BM25 custom','TFIDF nostem', 'TFIDF stem', 'TFIDF custom','BB2 nostem', 'BB2 stem', 'BB2 custom'],\n",
    "        round=4\n",
    "    )\n",
    "    \n",
    "    \n",
    "    result[config_data[\"lang\"]]={\"title_result\":title_result.to_dict(),\"doc_result\":doc_result.to_dict()}\n",
    "    \n",
    "    with open('./data_result.pkl', 'wb') as f:\n",
    "        pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18abd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "1:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/en59k/test\",\n",
    "        \"lang\":\"english\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"EnglishSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "2:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/es13k/test\",\n",
    "        \"lang\":\"spanish\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"SpanishSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "3:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/fr14k/test\",\n",
    "        \"lang\":\"french\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"FrenchSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "4:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/it16k/test\",\n",
    "        \"lang\":\"italian\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"ItalianSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    }\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a4f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda7acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1158036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51949a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f353aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████▊                    | 1/4 [20:29<1:01:28, 1229.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████               | 2/4 [34:13<33:01, 990.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████▌       | 3/4 [47:49<15:11, 911.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 4/4 [1:01:54<00:00, 928.68s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm(range(1,5)):\n",
    "    print(i)\n",
    "    config_data=config[i]\n",
    "    result[config_data[\"lang\"]]={}\n",
    "\n",
    "    custom_stemmer=None\n",
    "    custom_stopwords=None\n",
    "\n",
    "    dataset = pt.get_dataset(config_data[\"dataset\"])\n",
    "    custom_stemmer = SnowballStemmer(config_data[\"lang\"])\n",
    "    custom_stopwords = set(stopwords.words(config_data[\"lang\"]))\n",
    "    def custom_preprocess(text):\n",
    "        toks = word_tokenize(text) # tokenize\n",
    "        toks = [t for t in toks if t.lower() not in custom_stopwords] # remove stop words\n",
    "        toks = [custom_stemmer.stem(t) for t in toks] # stem\n",
    "        return ' '.join(toks) # combine toks back into a string\n",
    "\n",
    "    def map_doc(document,custom_preprocess=custom_preprocess):\n",
    "        # this function replaces the document text with the version that uses our custom pre-processing\n",
    "        return {\n",
    "            'docno': document['docno'],\n",
    "            'text': custom_preprocess(document['text'])\n",
    "        }\n",
    "\n",
    "    index_nostem = IndexCreator(dataset=dataset,filename=f'./wikir-{config_data[\"lang\"]}-nostem',\n",
    "                                tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None)\n",
    "    index_stem=IndexCreator(dataset=dataset,filename=f'./wikir-{config_data[\"lang\"]}-stem', \n",
    "                            tokeniser=\"UTFTokeniser\",stemmer=config_data[\"stemmer\"],stopwords=None)\n",
    "    index_custom = CustomIndexCreator(filename=f'./wikir-{config_data[\"lang\"]}-custom_stem', tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None,mapper=map_doc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    bm25_nostem = pt.BatchRetrieve(index_nostem, wmodel='BM25')\n",
    "    bm25_stem = pt.BatchRetrieve(index_stem, wmodel='BM25')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    bm25_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='BM25')\n",
    "\n",
    "\n",
    "    tfidf_nostem = pt.BatchRetrieve(index_nostem, wmodel='TF_IDF')\n",
    "    tfidf_stem = pt.BatchRetrieve(index_stem, wmodel='TF_IDF')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    tfidf_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='TF_IDF')\n",
    "\n",
    "\n",
    "\n",
    "    bb2_nostem = pt.BatchRetrieve(index_nostem, wmodel='BB2')\n",
    "    bb2_stem = pt.BatchRetrieve(index_stem, wmodel='BB2')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    bb2_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='BB2')\n",
    "\n",
    "\n",
    "\n",
    "    # http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html\n",
    "\n",
    "\n",
    "    title_qrels = dataset.get_qrels().copy()\n",
    "    title_qrels.loc[title_qrels.label < 2, 'label'] = 0\n",
    "    title_result=pt.Experiment(\n",
    "        [bm25_nostem, bm25_stem, bm25_custom,tfidf_nostem, tfidf_stem, tfidf_custom,bb2_nostem, bb2_stem, bb2_custom],\n",
    "        dataset.get_topics(),\n",
    "        title_qrels,\n",
    "        [nDCG@5, nDCG@10, nDCG@20,\"P_5\",\"P_10\",\"P_15\",\"P_20\",\"P_100\",\"recip_rank\",\"map\", NumQ],\n",
    "        names=['BM25 nostem', 'BM25 stem', 'BM25 custom','TFIDF nostem', 'TFIDF stem', 'TFIDF custom','BB2 nostem', 'BB2 stem', 'BB2 custom'],\n",
    "        round=4\n",
    "    )\n",
    "\n",
    "\n",
    "    doc_result=pt.Experiment(\n",
    "        [bm25_nostem, bm25_stem, bm25_custom,tfidf_nostem, tfidf_stem, tfidf_custom,bb2_nostem, bb2_stem, bb2_custom],\n",
    "        dataset.get_topics(),\n",
    "        dataset.get_qrels(),\n",
    "        [nDCG@5, nDCG@10, nDCG@20,\"P_5\",\"P_10\",\"P_15\",\"P_20\",\"P_100\",\"recip_rank\",\"map\", NumQ],\n",
    "        names=['BM25 nostem', 'BM25 stem', 'BM25 custom','TFIDF nostem', 'TFIDF stem', 'TFIDF custom','BB2 nostem', 'BB2 stem', 'BB2 custom'],\n",
    "        round=4\n",
    "    )\n",
    "    \n",
    "    \n",
    "    result[config_data[\"lang\"]]={\"title_result\":title_result.to_dict(),\"doc_result\":doc_result.to_dict()}\n",
    "    \n",
    "    with open('./data_result.pkl', 'wb') as f:\n",
    "        pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fa76c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def IndexCreator(dataset,filename, tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None):\n",
    "    if not os.path.exists(filename):\n",
    "        indexer = pt.IterDictIndexer(filename, \n",
    "            stemmer=stemmer, stopwords=stopwords, # Removes the default PorterStemmer (English)\n",
    "            tokeniser=tokeniser) # Replaces the default EnglishTokeniser, which makes assumptions specific to English\n",
    "        index_ = indexer.index(dataset.get_corpus_iter())\n",
    "    else:\n",
    "        index_ = pt.IndexRef.of(filename)\n",
    "    return index_\n",
    "\n",
    "\n",
    "def custom_preprocess(text):\n",
    "    toks = word_tokenize(text) # tokenize\n",
    "    toks = [t for t in toks if t.lower() not in custom_stopwords] # remove stop words\n",
    "    toks = [custom_stemmer.stem(t) for t in toks] # stem\n",
    "    return ' '.join(toks) # combine toks back into a string\n",
    "\n",
    "\n",
    "\n",
    "# Custom Preprocessing\n",
    "# NB: This custom pre-processing ends up being considerably slower than using Terrier's built-in processor,\n",
    "# so we use the multiprocessing package to parallelize (400 docs/s vs 2000 docs/s).\n",
    "def map_doc(document,custom_preprocess=custom_preprocess):\n",
    "    # this function replaces the document text with the version that uses our custom pre-processing\n",
    "    return {\n",
    "        'docno': document['docno'],\n",
    "        'text': custom_preprocess(document['text'])\n",
    "    }\n",
    "\n",
    "def CustomIndexCreator(filename, tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None,mapper=map_doc):\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        indexer = pt.IterDictIndexer(filename, \n",
    "            stemmer=None, stopwords=None,  # Disable the default PorterStemmer (English)\n",
    "            tokeniser=tokeniser) # Replaces the default EnglishTokeniser, which makes assumptions specific to English\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            index_custom = indexer.index(pool.imap(mapper, dataset.get_corpus_iter()))\n",
    "    else:\n",
    "        index_custom = pt.IndexRef.of(filename)\n",
    "    return index_custom\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "1:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/en59k/test\",\n",
    "        \"lang\":\"english\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"EnglishSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "2:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/es13k/test\",\n",
    "        \"lang\":\"spanish\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"SpanishSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "3:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/fr14k/test\",\n",
    "        \"lang\":\"french\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"FrenchSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    },\n",
    "4:\n",
    "    {\n",
    "        \"dataset\":\"irds:wikir/it16k/test\",\n",
    "        \"lang\":\"italian\",\n",
    "        \"tokeniser\":\"\",\n",
    "        \"stemmer\":\"ItalianSnowballStemmer\",\n",
    "        \"stopwords\":None\n",
    "    }\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f878c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "result={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c6cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778db3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869b044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████▊                    | 1/4 [20:29<1:01:28, 1229.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████               | 2/4 [34:13<33:01, 990.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████▌       | 3/4 [47:49<15:11, 911.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 4/4 [1:01:54<00:00, 928.68s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm(range(1,5)):\n",
    "    print(i)\n",
    "    config_data=config[i]\n",
    "    result[config_data[\"lang\"]]={}\n",
    "\n",
    "    custom_stemmer=None\n",
    "    custom_stopwords=None\n",
    "\n",
    "    dataset = pt.get_dataset(config_data[\"dataset\"])\n",
    "    custom_stemmer = SnowballStemmer(config_data[\"lang\"])\n",
    "    custom_stopwords = set(stopwords.words(config_data[\"lang\"]))\n",
    "    def custom_preprocess(text):\n",
    "        toks = word_tokenize(text) # tokenize\n",
    "        toks = [t for t in toks if t.lower() not in custom_stopwords] # remove stop words\n",
    "        toks = [custom_stemmer.stem(t) for t in toks] # stem\n",
    "        return ' '.join(toks) # combine toks back into a string\n",
    "\n",
    "    def map_doc(document,custom_preprocess=custom_preprocess):\n",
    "        # this function replaces the document text with the version that uses our custom pre-processing\n",
    "        return {\n",
    "            'docno': document['docno'],\n",
    "            'text': custom_preprocess(document['text'])\n",
    "        }\n",
    "\n",
    "    index_nostem = IndexCreator(dataset=dataset,filename=f'./wikir-{config_data[\"lang\"]}-nostem',\n",
    "                                tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None)\n",
    "    index_stem=IndexCreator(dataset=dataset,filename=f'./wikir-{config_data[\"lang\"]}-stem', \n",
    "                            tokeniser=\"UTFTokeniser\",stemmer=config_data[\"stemmer\"],stopwords=None)\n",
    "    index_custom = CustomIndexCreator(filename=f'./wikir-{config_data[\"lang\"]}-custom_stem', tokeniser=\"UTFTokeniser\",stemmer=None,stopwords=None,mapper=map_doc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    bm25_nostem = pt.BatchRetrieve(index_nostem, wmodel='BM25')\n",
    "    bm25_stem = pt.BatchRetrieve(index_stem, wmodel='BM25')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    bm25_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='BM25')\n",
    "\n",
    "\n",
    "    tfidf_nostem = pt.BatchRetrieve(index_nostem, wmodel='TF_IDF')\n",
    "    tfidf_stem = pt.BatchRetrieve(index_stem, wmodel='TF_IDF')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    tfidf_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='TF_IDF')\n",
    "\n",
    "\n",
    "\n",
    "    bb2_nostem = pt.BatchRetrieve(index_nostem, wmodel='BB2')\n",
    "    bb2_stem = pt.BatchRetrieve(index_stem, wmodel='BB2')\n",
    "    # to apply the es_preprocess function to the query text, use a pt.apply.query transformer\n",
    "    bb2_custom = pt.apply.query(lambda row: custom_preprocess(row.query)) >> pt.BatchRetrieve(index_custom, wmodel='BB2')\n",
    "\n",
    "\n",
    "\n",
    "    # http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html\n",
    "\n",
    "\n",
    "    title_qrels = dataset.get_qrels().copy()\n",
    "    title_qrels.loc[title_qrels.label < 2, 'label'] = 0\n",
    "    title_result=pt.Experiment(\n",
    "        [bm25_nostem, bm25_stem, bm25_custom,tfidf_nostem, tfidf_stem, tfidf_custom,bb2_nostem, bb2_stem, bb2_custom],\n",
    "        dataset.get_topics(),\n",
    "        title_qrels,\n",
    "        [nDCG@5, nDCG@10, nDCG@20,\"P_5\",\"P_10\",\"P_15\",\"P_20\",\"P_100\",\"recip_rank\",\"map\", NumQ],\n",
    "        names=['BM25 nostem', 'BM25 stem', 'BM25 custom','TFIDF nostem', 'TFIDF stem', 'TFIDF custom','BB2 nostem', 'BB2 stem', 'BB2 custom'],\n",
    "        round=4\n",
    "    )\n",
    "\n",
    "\n",
    "    doc_result=pt.Experiment(\n",
    "        [bm25_nostem, bm25_stem, bm25_custom,tfidf_nostem, tfidf_stem, tfidf_custom,bb2_nostem, bb2_stem, bb2_custom],\n",
    "        dataset.get_topics(),\n",
    "        dataset.get_qrels(),\n",
    "        [nDCG@5, nDCG@10, nDCG@20,\"P_5\",\"P_10\",\"P_15\",\"P_20\",\"P_100\",\"recip_rank\",\"map\", NumQ],\n",
    "        names=['BM25 nostem', 'BM25 stem', 'BM25 custom','TFIDF nostem', 'TFIDF stem', 'TFIDF custom','BB2 nostem', 'BB2 stem', 'BB2 custom'],\n",
    "        round=4\n",
    "    )\n",
    "    \n",
    "    \n",
    "    result[config_data[\"lang\"]]={\"title_result\":title_result.to_dict(),\"doc_result\":doc_result.to_dict()}\n",
    "    \n",
    "    with open('./data_result.pkl', 'wb') as f:\n",
    "        pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8cdf06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e74b3-7b5d-4f44-bb7a-510b758bb177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
